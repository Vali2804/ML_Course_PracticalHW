{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing\n",
    "\n",
    "## a. Dataset Description\n",
    "- Provide a brief description of the dataset, including details about its attributes and target attribute. Clearly state the purpose of the dataset. Identify and categorize attributes as discrete or continuous.\n",
    "\n",
    "## b. Handling NaN Values\n",
    "- Identify and handle NaN values in the dataset. Remove rows that contain NaN values to ensure data integrity.\n",
    "\n",
    "## c. Numerical Attribute Statistics\n",
    "- Calculate the mean and variance for each numerical attribute in the dataset. This step helps in understanding the central tendency and spread of the numerical features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean values for each attribute:\n",
      " bill_length_mm         43.992793\n",
      "bill_depth_mm          17.164865\n",
      "flipper_length_mm     200.966967\n",
      "body_mass_g          4207.057057\n",
      "dtype: float64\n",
      "Variance values for each attribute:\n",
      " bill_length_mm           29.906333\n",
      "bill_depth_mm             3.877888\n",
      "flipper_length_mm       196.441677\n",
      "body_mass_g          648372.487699\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Dataset\n",
    "dataFrame = pd.read_csv('https://raw.githubusercontent.com/Vali2804/ML_setDate/main/penguins.csv')\n",
    "\n",
    "# 1. Preprocessing\n",
    "# Dataset Description \n",
    "\n",
    "#a.\n",
    "# Penguin Dataset Description\n",
    "attributes = ['species', 'island', 'bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']\n",
    "targetAttribute = 'species'\n",
    "purpose = 'Classification of penguin species'\n",
    "\n",
    "# Attribute types\n",
    "discreteAttributes = ['species', 'island']\n",
    "continuousAttributes = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']\n",
    "\n",
    "# b.\n",
    "# Remove NaN values\n",
    "dataFrame = dataFrame.dropna()\n",
    "\n",
    "# c.\n",
    "# Calculte mean and variance for each attribute\n",
    "num_attributes = dataFrame[['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']]\n",
    "mean_values = num_attributes.mean()\n",
    "variance_values = num_attributes.var()\n",
    "\n",
    "print('Mean values for each attribute:\\n', mean_values)\n",
    "print('Variance values for each attribute:\\n', variance_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Weak estimators\n",
    "## a. Data Preprocessing - Handling Non-Numeric Discrete Attributes\n",
    "- Convert the discrete attributes that are not numeric (such as strings or boolean values) into numerical. If this doesn't apply to your dataset, provide\n",
    "a short explanation on how you would proceed.\n",
    "\n",
    "## b.  Custom Function for Variable Discretization and Testing with Example\n",
    "- Write a function get_splits which, given an attribute and the labels column, will identify the splits that could be used to discretization of the\n",
    "variable. Test your function on an example.\n",
    "\n",
    "## c. AdaBoost Configuration - Counting Weak Estimators and External Threshold\n",
    "- How many weak estimators (or splits) are available in our version of AdaBoost? (include the external threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete attributes converted to numeric:\n",
      "      species  island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
      "0          0       2            39.1           18.7              181.0   \n",
      "1          0       2            39.5           17.4              186.0   \n",
      "2          0       2            40.3           18.0              195.0   \n",
      "4          0       2            36.7           19.3              193.0   \n",
      "5          0       2            39.3           20.6              190.0   \n",
      "..       ...     ...             ...            ...                ...   \n",
      "338        2       0            47.2           13.7              214.0   \n",
      "340        2       0            46.8           14.3              215.0   \n",
      "341        2       0            50.4           15.7              222.0   \n",
      "342        2       0            45.2           14.8              212.0   \n",
      "343        2       0            49.9           16.1              213.0   \n",
      "\n",
      "     body_mass_g     sex  \n",
      "0         3750.0    MALE  \n",
      "1         3800.0  FEMALE  \n",
      "2         3250.0  FEMALE  \n",
      "4         3450.0  FEMALE  \n",
      "5         3650.0    MALE  \n",
      "..           ...     ...  \n",
      "338       4925.0  FEMALE  \n",
      "340       4850.0  FEMALE  \n",
      "341       5750.0    MALE  \n",
      "342       5200.0  FEMALE  \n",
      "343       5400.0    MALE  \n",
      "\n",
      "[333 rows x 7 columns]\n",
      "Splits for attribute species:\n",
      " [(0, 0.6920921544209215), (1, 0.7811320754716982), (2, 0.5070093457943925)]\n"
     ]
    }
   ],
   "source": [
    "# 2. Weak estimators\n",
    "# a. convert discrete attributes to numeric\n",
    "dataFrame['species'] = dataFrame['species'].astype('category').cat.codes\n",
    "dataFrame['island'] = dataFrame['island'].astype('category').cat.codes\n",
    "\n",
    "print('Discrete attributes converted to numeric:\\n', dataFrame)\n",
    "# b. the function get_splits()\n",
    "def get_splits(attribute_values, labels):\n",
    "    unique_values = np.unique(attribute_values)\n",
    "    splits = []\n",
    "\n",
    "    for value in unique_values:\n",
    "        left_mask = attribute_values == value\n",
    "        right_mask = ~left_mask\n",
    "\n",
    "        left_labels = labels[left_mask]\n",
    "        right_labels = labels[right_mask]\n",
    "\n",
    "        split = (left_labels.mean() + right_labels.mean()) / 2\n",
    "        splits.append((value, split))\n",
    "\n",
    "    return splits\n",
    "\n",
    "# testing get_splits()\n",
    "attribute_values = dataFrame['species'].values\n",
    "labels = dataFrame['island'].values\n",
    "splits = get_splits(attribute_values, labels)\n",
    "print('Splits for attribute species:\\n', splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. AdaBoost\n",
    "# a. The function calculate_split_probs\n",
    "- Write a function calculate_split_probs that takes the following arguments:\n",
    "    + attribute - the name of one attribute (a column from your dataframe)\n",
    "    + split - the value of the split\n",
    "    + D - the current distribution of weights for each instance of the training dataset ( sum(D) should be 1 )\n",
    "The function should return a dictionary following this structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split probabilities for attribute species:\n",
      " {'left': {0: 1.0, 1: 0.0, 2: 0.0}, 'right': {0: 0.0, 1: 0.3636363636363635, 2: 0.6363636363636362}}\n"
     ]
    }
   ],
   "source": [
    "def calculate_split_probs(attribute, split, D):\n",
    "    left_mask = attribute <= split\n",
    "    right_mask = attribute > split\n",
    "\n",
    "    left_weights = {}\n",
    "    right_weights = {}\n",
    "\n",
    "    for label in np.unique(attribute):\n",
    "        left_denominator = np.sum(D[left_mask])\n",
    "        right_denominator = np.sum(D[right_mask])\n",
    "\n",
    "        if left_denominator != 0:\n",
    "            left_weights[label] = np.sum(D[left_mask & (attribute == label)]) / left_denominator\n",
    "        else:\n",
    "            left_weights[label] = 0\n",
    "\n",
    "        if right_denominator != 0:\n",
    "            right_weights[label] = np.sum(D[right_mask & (attribute == label)]) / right_denominator\n",
    "        else:\n",
    "            right_weights[label] = 0\n",
    "\n",
    "    return {'left': left_weights, 'right': right_weights}\n",
    "\n",
    "# testing calculate_split_probs()\n",
    "attribute = dataFrame['species'].values\n",
    "split = 0.5\n",
    "D = np.ones(len(attribute)) / len(attribute)\n",
    "split_probs = calculate_split_probs(attribute, split, D)\n",
    "print('Split probabilities for attribute species:\\n', split_probs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b. The function calculate_split_prediction \n",
    "- Write a function calculate_split_prediction that takes the output of calculate_split_probs and returns a vector [label_left, label_right] ,\n",
    "where label_left is the label with the highest weight on the left side of the split, and label_right is the label with the highest weight on the right\n",
    "side of the split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split prediction for attribute species:\n",
      " [0, 2]\n"
     ]
    }
   ],
   "source": [
    "def calculate_split_prediction(split_probs):\n",
    "    left_weights = split_probs['left']\n",
    "    right_weights = split_probs['right']\n",
    "\n",
    "    label_left = max(left_weights, key=lambda x: left_weights[x])\n",
    "    label_right = max(right_weights, key=lambda x: right_weights[x])\n",
    "\n",
    "    return [label_left, label_right]\n",
    "# testing calculate_split_prediction()\n",
    "split_prediction = calculate_split_prediction(split_probs)\n",
    "print('Split prediction for attribute species:\\n', split_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c. The function calculate_split_errors \n",
    "- Write a function calculate_split_errors that takes the output of calculate_split_probs and returns the error of the split, i.e. 1 - the largest weight\n",
    "on the left side of the split - the largest weight on the right side of the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split errors for attribute species:\n",
      " (0.6363636363636362, 0.0, 0.36363636363636376)\n"
     ]
    }
   ],
   "source": [
    "def calculate_split_errors(split_probs):\n",
    "    left_weights = split_probs['left']\n",
    "    right_weights = split_probs['right']\n",
    "\n",
    "    max_left_weight = max(left_weights, key=lambda x: left_weights[x])\n",
    "    max_right_weight = max(right_weights, key=lambda x: right_weights[x])\n",
    "\n",
    "    error = abs(1 - left_weights[max_left_weight] - right_weights[max_right_weight])\n",
    "    left_error = abs(1 - left_weights[max_left_weight])\n",
    "    right_error = abs(1 - right_weights[max_right_weight])\n",
    "    return error, left_error, right_error\n",
    "\n",
    "# testing calculate_split_errors()\n",
    "split_errors = calculate_split_errors(split_probs)\n",
    "print('Split errors for attribute species:\\n', split_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# d. The function find_best_estimator \n",
    "- Write a function find_best_estimator that, given a dataframe df , a list of splits for each attribute splits_list and a distribution of weights D , will\n",
    "find the attribute and split that achieves the lowest error.\n",
    "The return of the function should be the following: [(index_of_best_attribute, best_split, label_left_side, label_right_side),\n",
    "estimator_error] .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Estimator Information:\n",
      "(best_attribute_index, best_split_value, best_label_left, best_label_right)\n",
      " [(0, 32.1, 32.1, 41.1), 0.021084337349397585]\n"
     ]
    }
   ],
   "source": [
    "def find_best_estimator(df, splits_list, D):\n",
    "    best_attribute_index = None\n",
    "    best_split_value = None\n",
    "    best_label_left = None\n",
    "    best_label_right = None\n",
    "    best_estimator_error = float('inf')\n",
    "\n",
    "    for i, attribute in enumerate(continuousAttributes):\n",
    "        attribute_values = df[attribute].values\n",
    "        labels = df[targetAttribute].values\n",
    "\n",
    "        for split_value, _ in splits_list[attribute]:\n",
    "            split_probs = calculate_split_probs(attribute_values, split_value, D)\n",
    "            split_errors = calculate_split_errors(split_probs)\n",
    "\n",
    "            total_error = split_errors[0]\n",
    "\n",
    "            if total_error < best_estimator_error:\n",
    "                best_attribute_index = i\n",
    "                best_split_value = split_value\n",
    "                best_label_left, best_label_right = calculate_split_prediction(split_probs)\n",
    "                best_estimator_error = total_error\n",
    "\n",
    "    return [(best_attribute_index, best_split_value, best_label_left, best_label_right), best_estimator_error]\n",
    "\n",
    "# Testing the find_best_estimator function\n",
    "splits_list = {attribute: get_splits(dataFrame[attribute].values, dataFrame[targetAttribute].values) for attribute in continuousAttributes}\n",
    "weights_distribution = np.ones(len(dataFrame)) / len(dataFrame)\n",
    "best_estimator_info = find_best_estimator(dataFrame, splits_list, weights_distribution)\n",
    "print('Best Estimator Information:\\n(best_attribute_index, best_split_value, best_label_left, best_label_right)\\n', best_estimator_info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# e.   \n",
    "- Assuming that D follows the uniform distribution, what is the attribute and the split that achieves the lowest error? What are the labels predicted by\n",
    "the split?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# f. The function get_estimator_weight \n",
    "- Write a function get_estimator_weight that will calculate the weight alpha of the estimator determined at point d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Weight:\n",
      " 1.9189575166372121\n"
     ]
    }
   ],
   "source": [
    "def get_estimator_weight(estimator_error):\n",
    "    return 0.5 * np.log((1 - estimator_error) / estimator_error)\n",
    "# testing the get_estimator_weight function\n",
    "estimator_weight = get_estimator_weight(best_estimator_info[1])\n",
    "print('Estimator Weight:\\n', estimator_weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# g. The function update_D\n",
    "- Write a function update_D that, given a dataframe df , an estimator current_h and the current distribution D , will return the vector with the updated\n",
    "weights of the training instances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated D:\n",
      " [0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003\n",
      " 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003\n",
      " 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003\n",
      " 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003\n",
      " 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003\n",
      " 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003\n",
      " 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003\n",
      " 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003\n",
      " 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003\n",
      " 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003\n",
      " 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003\n",
      " 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003\n",
      " 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003\n",
      " 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003\n",
      " 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003\n",
      " 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003\n",
      " 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003\n",
      " 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003\n",
      " 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003\n",
      " 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003\n",
      " 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003\n",
      " 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003\n",
      " 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003\n",
      " 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003\n",
      " 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003\n",
      " 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003\n",
      " 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003\n",
      " 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003\n",
      " 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003\n",
      " 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003\n",
      " 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003\n",
      " 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003\n",
      " 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003\n",
      " 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003\n",
      " 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003\n",
      " 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003\n",
      " 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003\n",
      " 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003\n",
      " 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003\n",
      " 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003\n",
      " 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003 0.003003\n",
      " 0.003003 0.003003 0.003003 0.003003 0.003003]\n"
     ]
    }
   ],
   "source": [
    "def update_D(df, current_h, D):\n",
    "    attribute = continuousAttributes[current_h[0]]\n",
    "    split_value = current_h[1]\n",
    "    label_left = current_h[2]\n",
    "    label_right = current_h[3]\n",
    "\n",
    "    attribute_values = df[attribute].values\n",
    "    labels = df[targetAttribute].values\n",
    "\n",
    "    left_mask = attribute_values <= split_value\n",
    "    right_mask = attribute_values > split_value\n",
    "\n",
    "    left_labels = labels[left_mask]\n",
    "    right_labels = labels[right_mask]\n",
    "\n",
    "    D_left = D[left_mask]\n",
    "    D_right = D[right_mask]\n",
    "\n",
    "    D_updated = np.zeros(len(D))\n",
    "\n",
    "    D_updated[left_mask] = D_left * np.exp(-estimator_weight * (left_labels != label_left))\n",
    "    D_updated[right_mask] = D_right * np.exp(-estimator_weight * (right_labels != label_right))\n",
    "\n",
    "    D_updated /= np.sum(D_updated)\n",
    "\n",
    "    return D_updated\n",
    "\n",
    "#testing the update_D function\n",
    "D_updated = update_D(dataFrame, best_estimator_info[0], weights_distribution)\n",
    "print('Updated D:\\n', D_updated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# h. The function adaboost \n",
    "- Write a function adaboost that, given a dataframe df and a number of iterations niter , will train an AdaBoost model. The function should return\n",
    "the following dictionary \n",
    "```python\n",
    "{\n",
    "\"estimators\": the list of the `niter` estimators; each estimator should be described as (index_of_best_attribute, best_split,\n",
    "label_left_side, label_right_side)\n",
    "\"estimators_weights\": the list of the estimators' weights\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Information:\n",
      " {'estimators': [(0, 32.1, 32.1, 41.1), (0, 32.1, 32.1, 41.1), (0, 32.1, 32.1, 41.1), (0, 32.1, 32.1, 41.1), (0, 32.1, 32.1, 41.1), (0, 32.1, 32.1, 41.1), (0, 32.1, 32.1, 41.1), (0, 32.1, 32.1, 41.1), (0, 32.1, 32.1, 41.1), (0, 32.1, 32.1, 41.1)], 'estimators_weights': [1.9189575166372121, 1.9189575166372121, 1.9189575166372121, 1.9189575166372121, 1.9189575166372121, 1.9189575166372121, 1.9189575166372121, 1.9189575166372121, 1.9189575166372121, 1.9189575166372121]}\n"
     ]
    }
   ],
   "source": [
    "def adaboost(df, niter):\n",
    "    # Initialize weights\n",
    "    D = np.ones(len(df)) / len(df)\n",
    "\n",
    "    # Lists to store estimators and their weights\n",
    "    estimators = []\n",
    "    estimators_weights = []\n",
    "\n",
    "    for _ in range(niter):\n",
    "        # Get splits for continuous attributes\n",
    "        splits_list = {attribute: get_splits(df[attribute].values, df[targetAttribute].values) for attribute in continuousAttributes}\n",
    "\n",
    "        # Find the best weak estimator\n",
    "        best_estimator_info = find_best_estimator(df, splits_list, D)\n",
    "\n",
    "        # Get the weight for the weak estimator\n",
    "        estimator_weight = get_estimator_weight(best_estimator_info[1])\n",
    "\n",
    "        # Update the weights distribution\n",
    "        D = update_D(df, best_estimator_info[0], D)\n",
    "\n",
    "        # Store the weak estimator and its weight\n",
    "        estimators.append(best_estimator_info[0])\n",
    "        estimators_weights.append(estimator_weight)\n",
    "\n",
    "    return {\"estimators\": estimators, \"estimators_weights\": estimators_weights}\n",
    "# testing the adaboost function\n",
    "adaboost_info = adaboost(dataFrame, 10)\n",
    "print('AdaBoost Information:\\n', adaboost_info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# i. The function adaboost_predict\n",
    "- Write a function adaboost_predict that, given an instance X and the model created at h, will return the label predicted by the majority of the\n",
    "estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for the first instance:\n",
      " 32.1\n"
     ]
    }
   ],
   "source": [
    "def adaboost_predict(X, model):\n",
    "    # Initialize a dictionary to store the votes for each label\n",
    "    label_votes = {}\n",
    "\n",
    "    # Iterate through each weak estimator in the model\n",
    "    for i, estimator_info in enumerate(model[\"estimators\"]):\n",
    "        # Get the information about the current weak estimator\n",
    "        attribute_index, split_value, label_left, label_right = estimator_info\n",
    "\n",
    "        # Get the attribute value for the current instance\n",
    "        attribute_value = X.iloc[attribute_index]\n",
    "\n",
    "        # Make a prediction based on the weak estimator\n",
    "        if attribute_value <= split_value:\n",
    "            label = label_left\n",
    "        else:\n",
    "            label = label_right\n",
    "\n",
    "        # Update the votes for the predicted label\n",
    "        if label in label_votes:\n",
    "            label_votes[label] += model[\"estimators_weights\"][i]\n",
    "        else:\n",
    "            label_votes[label] = model[\"estimators_weights\"][i]\n",
    "\n",
    "    # Return the label with the majority of votes\n",
    "    return max(label_votes, key=label_votes.get)\n",
    "\n",
    "# testing adaboost_predict function\n",
    "X = dataFrame.iloc[0]\n",
    "model = adaboost(dataFrame, 10)\n",
    "prediction = adaboost_predict(X, model)\n",
    "\n",
    "print('Prediction for the first instance:\\n', prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# j. The function cross_validation\n",
    "- Use cross-validation to identify the appropriate number of iterations that will achieve good results on your dataset. The number of iterations shouldn't\n",
    "exceed 50.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
